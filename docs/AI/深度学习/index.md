# 深度学习

在深度学习的世界里，RNN、LSTM 和 CNN 是三驾马车。它们各有各的“性格”和擅长领域：**CNN 擅长空间（看图），RNN 擅长序列（听歌/读文章），而 LSTM 则是 RNN 的“记性增强版”。**

---

## 1. CNN (卷积神经网络)

CNN 的核心逻辑是**局部感应**。它模仿人类视觉，不是一眼看完全图，而是通过一个个“小窗口”扫描图像。

* **核心组件：**
* **卷积层 (Convolutional Layer)：** 像是一个带焦距的放大镜（滤波器/卷积核），在图片上滑动，提取边缘、纹理等特征。
* **池化层 (Pooling Layer)：** 负责“压缩”。比如把一张  的图变成 ，保留最明显的特征（如最大值），减少计算量。
* **全连接层 (Fully Connected Layer)：** 把最后提取到的特征拼在一起，做最终的分类（比如判断这是猫还是狗）。


* **适用场景：** 图像识别、人脸识别、视频分析。

---

## 2. RNN (循环神经网络)

RNN 专门处理**序列数据**。它的特点是“有记忆”，当前这一秒的输出不仅取决于当前的输入，还取决于上一秒留下的“念想”。

* **核心逻辑：**
它的神经元之间形成了一个环路。处理一段话时，它会一个词一个词地读，每读一个词，都会更新自己的**隐藏状态 (Hidden State)**。
* **致命弱点：**
**“短时记忆”**。由于数学上的“梯度消失”问题，RNN 读到句子结尾时，往往已经忘了开头讲了什么。这就像读一本长小说，看到第十章已经忘了第一章主角的名字。
* **适用场景：** 短文本预测、简单的时序分析。

---

## 3. LSTM (长短期记忆网络)

LSTM 是为了解决 RNN 的“健忘症”而诞生的。它本质上也是一种 RNN，但内部结构极其精巧，增加了一个**状态通路**。

* **三个“门”控机制：**
1. **遗忘门 (Forget Gate)：** 决定哪些旧信息没用了，该扔掉（比如换了一个话题，旧话题背景就该清空）。
2. **输入门 (Input Gate)：** 决定哪些新信息值得存进“笔记本”。
3. **输出门 (Output Gate)：** 决定在当前的时刻，笔记本里的哪些内容该输出。


* **核心优势：**
它能通过这三个门，长久地保留重要信息，过滤掉无用噪音。
* **适用场景：** 机器翻译、语音识别、股票预测。

---

## 三者对比总结

| 特性 | CNN | RNN | LSTM |
| --- | --- | --- | --- |
| **数据类型** | 空间数据（图像、视频） | 序列数据（文本、音频） | 长序列数据（长文章、长语音） |
| **主要优势** | 提取局部特征，参数共享 | 处理变长序列，有上下文意识 | 解决长距离依赖问题，记性好 |
| **类比理解** | 像**扫描仪**，一块一块看 | 像**录音机**，边听边记 | 像**带橡皮擦的笔记本**，记重点 |
